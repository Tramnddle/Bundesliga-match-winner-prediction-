{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c2d37e-64ce-440a-a81e-c67c288495f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "standings_url=\"https://fbref.com/en/comps/20/2016-2017/2016-2017-Bundesliga-Stats\"\n",
    "years = list(range(2017, 2013, -1))\n",
    "all_matches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2320e811-97bc-42f8-9844-84528894f70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tramn\\AppData\\Local\\Temp\\ipykernel_4324\\1783270792.py:17: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  Scores_Fixtures = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
      "C:\\Users\\tramn\\AppData\\Local\\Temp\\ipykernel_4324\\1783270792.py:24: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  shooting = pd.read_html(data.text, match=\"Shooting\")[0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m links \u001b[38;5;241m=\u001b[39m [l\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     29\u001b[0m links \u001b[38;5;241m=\u001b[39m [l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m links \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_comps/defense/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m l]\n\u001b[1;32m---> 30\u001b[0m data \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://fbref.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlinks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m defense \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(data\u001b[38;5;241m.\u001b[39mtext, match\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefensive Actions\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     32\u001b[0m defense\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m defense\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdroplevel()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for year in years:\n",
    "    data = requests.get(standings_url)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "    links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "    links = [l for l in links if '/squads/' in l]\n",
    "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
    "    standings_url = f\"https://fbref.com{previous_season}\"\n",
    "    \n",
    "    for team_url in team_urls:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        data = requests.get(team_url)\n",
    "        Scores_Fixtures = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "        Scores_Fixtures = Scores_Fixtures[['Date','Time','Comp','Round','Day','Venue','GF','GA','Opponent','Poss']]\n",
    "        \n",
    "        soup = BeautifulSoup(data.text)\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        shooting = pd.read_html(data.text, match=\"Shooting\")[0]\n",
    "        shooting.columns = shooting.columns.droplevel()\n",
    "        \n",
    "        soup = BeautifulSoup(data.text)\n",
    "        links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "        links = [l for l in links if l and 'all_comps/defense/' in l]\n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        defense = pd.read_html(data.text, match=\"Defensive Actions\")[0]\n",
    "        defense.columns = defense.columns.droplevel()\n",
    "        \n",
    "        soup = BeautifulSoup(data.text)\n",
    "        links = soup.find_all('a')\n",
    "        links = [l.get(\"href\") for l in links]\n",
    "        links = [l for l in links if l and 'all_comps/keeper/' in l]\n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        goalkeeping = pd.read_html(data.text, match=\"Goalkeeping\")[0]\n",
    "        goalkeeping.columns = goalkeeping.columns.droplevel()\n",
    "\n",
    "        soup = BeautifulSoup(data.text)\n",
    "        links = soup.find_all('a')\n",
    "        links = [l.get(\"href\") for l in links]\n",
    "        links = [l for l in links if l and 'all_comps/passing/' in l]\n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        passing = pd.read_html(data.text, match=\"Passing\")[0]\n",
    "        upper_level = passing.columns.get_level_values(0)\n",
    "        new_columns = [f'{prefix}_{col}' for prefix, col in zip(upper_level, passing.columns.get_level_values(1))]\n",
    "        passing.columns = new_columns\n",
    "        passing.rename(columns={passing.columns[0]: 'Date'}, inplace=True)\n",
    "\n",
    "        soup = BeautifulSoup(data.text)\n",
    "        links = soup.find_all('a')\n",
    "        links = [l.get(\"href\") for l in links]\n",
    "        links = [l for l in links if l and 'all_comps/possession/' in l]\n",
    "        data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "        possession = pd.read_html(data.text, match=\"Possession\")[0]\n",
    "        possession.columns = possession.columns.droplevel()\n",
    "        \n",
    "        #try:\n",
    "        team_data = Scores_Fixtures.merge(shooting[['Date','Sh']], on = ['Date'])\n",
    "        team_data = team_data.merge(goalkeeping[['Date','Save%']], on = ['Date'])\n",
    "        team_data = team_data.merge(passing[['Date','Total_Att','Total_Cmp%']], on = ['Date'])\n",
    "        team_data = team_data.merge(possession[['Date','Att']], on = ['Date'])\n",
    "        team_data = team_data.merge(defense[['Date','Blocks']], on = ['Date'])\n",
    "        #except ValueError:\n",
    "            #continue\n",
    "        team_data = team_data[team_data[\"Comp\"] == \"Bundesliga\"]\n",
    "        \n",
    "        team_data[\"Season\"] = year\n",
    "        team_data[\"Team\"] = team_name\n",
    "        all_matches.append(team_data)\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c77c5b-a443-44dd-a80c-ce0fcd7b7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3256116e-0a1c-43db-8cc4-9ac8d2648c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = pd.concat(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d14c2-0b4c-4091-b0ac-c8609eb01b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df.columns = [c.lower() for c in match_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7a8cf-10f9-4c1d-85b6-91d318e4dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88d360-1434-45f7-b7a3-1415a796978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df.to_csv(\"19_23_matches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf6cb5-3b5c-4e4f-a5f3-7f8af19e4e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
