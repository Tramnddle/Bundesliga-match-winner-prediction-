{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6479f85-3637-4367-951b-32a1ddd2ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fetch data\n",
    "def fetch_data(last_update_time):\n",
    "    standings_url = \"https://fbref.com/en/comps/20/Bundesliga-Stats\"\n",
    "    years = list(range(2024, 2023, -1))\n",
    "    all_matches = []\n",
    "\n",
    "    for year in years:\n",
    "        data = requests.get(standings_url)\n",
    "        soup = BeautifulSoup(data.text)\n",
    "        standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "        links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
    "        links = [l for l in links if '/squads/' in l]\n",
    "        team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
    "\n",
    "        previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
    "        standings_url = f\"https://fbref.com{previous_season}\"\n",
    "\n",
    "        for team_url in team_urls:\n",
    "            team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "            data = requests.get(team_url)\n",
    "            Scores_Fixtures = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "            Scores_Fixtures = Scores_Fixtures[['Date', 'Time', 'Comp', 'Round', 'Day', 'Venue', 'GF', 'GA', 'Opponent', 'Poss']]\n",
    "\n",
    "            team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "            data = requests.get(team_url)\n",
    "            Scores_Fixtures = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
    "            Scores_Fixtures = Scores_Fixtures[['Date','Time','Comp','Round','Day','Venue','GF','GA','Opponent','Poss']]\n",
    "            \n",
    "            soup = BeautifulSoup(data.text)\n",
    "            links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "            links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
    "            data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "            shooting = pd.read_html(data.text, match=\"Shooting\")[0]\n",
    "            shooting.columns = shooting.columns.droplevel()\n",
    "            \n",
    "            soup = BeautifulSoup(data.text)\n",
    "            links = [l.get(\"href\") for l in soup.find_all('a')]\n",
    "            links = [l for l in links if l and 'all_comps/defense/' in l]\n",
    "            data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "            defense = pd.read_html(data.text, match=\"Defensive Actions\")[0]\n",
    "            defense.columns = defense.columns.droplevel()\n",
    "            \n",
    "            soup = BeautifulSoup(data.text)\n",
    "            links = soup.find_all('a')\n",
    "            links = [l.get(\"href\") for l in links]\n",
    "            links = [l for l in links if l and 'all_comps/keeper/' in l]\n",
    "            data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "            goalkeeping = pd.read_html(data.text, match=\"Goalkeeping\")[0]\n",
    "            goalkeeping.columns = goalkeeping.columns.droplevel()\n",
    "    \n",
    "            soup = BeautifulSoup(data.text)\n",
    "            links = soup.find_all('a')\n",
    "            links = [l.get(\"href\") for l in links]\n",
    "            links = [l for l in links if l and 'all_comps/passing/' in l]\n",
    "            data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "            passing = pd.read_html(data.text, match=\"Passing\")[0]\n",
    "            upper_level = passing.columns.get_level_values(0)\n",
    "            new_columns = [f'{prefix}_{col}' for prefix, col in zip(upper_level, passing.columns.get_level_values(1))]\n",
    "            passing.columns = new_columns\n",
    "            passing.rename(columns={passing.columns[0]: 'Date'}, inplace=True)\n",
    "    \n",
    "            soup = BeautifulSoup(data.text)\n",
    "            links = soup.find_all('a')\n",
    "            links = [l.get(\"href\") for l in links]\n",
    "            links = [l for l in links if l and 'all_comps/possession/' in l]\n",
    "            data = requests.get(f\"https://fbref.com{links[0]}\")\n",
    "            possession = pd.read_html(data.text, match=\"Possession\")[0]\n",
    "            possession.columns = possession.columns.droplevel()\n",
    "        \n",
    "            #try:\n",
    "            team_data = Scores_Fixtures.merge(shooting[['Date','Sh']], on = ['Date'])\n",
    "            team_data = team_data.merge(goalkeeping[['Date','Save%']], on = ['Date'])\n",
    "            team_data = team_data.merge(passing[['Date','Total_Att','Total_Cmp%']], on = ['Date'])\n",
    "            team_data = team_data.merge(possession[['Date','Att']], on = ['Date'])\n",
    "            team_data = team_data.merge(defense[['Date','Blocks']], on = ['Date'])\n",
    "            \n",
    "            # Filter data based on last update time\n",
    "            team_data = Scores_Fixtures[Scores_Fixtures['Date'] > last_update_time]\n",
    "\n",
    "\n",
    "            team_data = team_data[team_data[\"Comp\"] == \"Bundesliga\"]\n",
    "            team_data[\"Season\"] = year\n",
    "            team_data[\"Team\"] = team_name\n",
    "            all_matches.append(team_data)\n",
    "            time.sleep(15)\n",
    "\n",
    "    # Combine all dataframes and save to a CSV file or process further\n",
    "    result_df = pd.concat(all_matches)\n",
    "    result_df.to_csv(\"weekly_data.csv\", index=False)\n",
    "\n",
    "    # Update last update time to the current time\n",
    "    return datetime.now()\n",
    "\n",
    "# Initialize last_update_time with a default or saved timestamp\n",
    "last_update_time = datetime(2023, 1, 1)\n",
    "\n",
    "# Schedule the fetch_data function to run every week\n",
    "schedule.every().week.do(fetch_data, last_update_time)\n",
    "\n",
    "# Run the scheduled jobs\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a0837b-49a4-4a61-a4da-4506ca6151f5",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "for year in years:\n",
    "    data = requests.get(standings_url)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    standings_table = soup.select('table.stats_table')[0]\n",
    "\n",
    "    links = [f\"https://fbref.com{l.get('href')}\" for l in standings_table.find_all('a') if '/squads/' in l.get('href')]\n",
    "    \n",
    "    previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
    "    standings_url = f\"https://fbref.com{previous_season}\"\n",
    "\n",
    "    for team_url in links:\n",
    "        team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
    "        data = requests.get(team_url)\n",
    "        \n",
    "        Scores_Fixtures = pd.read_html(data.text, match=\"Scores & Fixtures\")[0][['Date','Time','Comp','Round','Day','Venue','GF','GA','Opponent','Poss']]\n",
    "        shooting = pd.read_html(data.text, match=\"Shooting\")[0].droplevel(0, axis=1)\n",
    "        defense = pd.read_html(data.text, match=\"Defensive Actions\")[0].droplevel(0, axis=1)\n",
    "        goalkeeping = pd.read_html(data.text, match=\"Goalkeeping\")[0].droplevel(0, axis=1)\n",
    "        passing = pd.read_html(data.text, match=\"Passing\")[0].droplevel(0, axis=1)\n",
    "        possession = pd.read_html(data.text, match=\"Possession\")[0].droplevel(0, axis=1)\n",
    "\n",
    "        team_data = Scores_Fixtures.merge(shooting[['Date','Sh']], on='Date')\n",
    "        team_data = team_data.merge(goalkeeping[['Date','Save%']], on='Date')\n",
    "        team_data = team_data.merge(passing.rename(columns={passing.columns[0]: 'Date'}), on='Date')\n",
    "        team_data = team_data.merge(possession[['Date','Att']], on='Date')\n",
    "        team_data = team_data.merge(defense[['Date','Blocks']], on='Date')\n",
    "        \n",
    "        team_data = team_data[team_data[\"Comp\"] == \"Bundesliga\"]\n",
    "        team_data[\"Season\"] = year\n",
    "        team_data[\"Team\"] = team_name\n",
    "        all_matches.append(team_data)\n",
    "        \n",
    "        time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf1f236-9545-43f5-a1cb-27cfc1750586",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478e425-cf94-4c67-b15a-1ebb1f6ec53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df = pd.concat(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91280e71-9ad5-43c0-9571-7cce030ff2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df.columns = [c.lower() for c in match_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dcbaeb-a2d8-400f-817a-216f38eed2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04856dd3-a056-488e-b3a8-ace0e23c28b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
