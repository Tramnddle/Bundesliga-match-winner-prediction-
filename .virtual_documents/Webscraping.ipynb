import requests
import pandas as pd


standings_url="https://fbref.com/en/comps/20/Bundesliga-Stats"


data=requests.get(standings_url)


data.text


from bs4 import BeautifulSoup


soup = BeautifulSoup(data.text)


standings_table = soup.select('table.stats_table')[0]


standings_table


links = standings_table.find_all('a')


links = [l.get("href") for l in links]


links = [l for l in links if '/squads/' in l]


links


data = requests.get(f"https://fbref.com{links[0]}")


Scores_Fixtures = pd.read_html(data.text, match="Scores & Fixtures")[0]


Scores_Fixtures.head()


Scores_Fixtures.head()


soup = BeautifulSoup(data.text)
links = soup.find_all('a')
links = [l.get("href") for l in links]
links = [l for l in links if l and 'all_comps/shooting/' in l]
data = requests.get(f"https://fbref.com{links[0]}")
shooting = pd.read_html(data.text, match="Shooting")[0]
shooting.columns = shooting.columns.droplevel()


#shooting = shooting[['Date','Time','Sh','SoT']]
shooting.head()


soup = BeautifulSoup(data.text)
links = soup.find_all('a')
links = [l.get("href") for l in links]
links = [l for l in links if l and 'all_comps/keeper/' in l]
data = requests.get(f"https://fbref.com{links[0]}")
goalkeeping = pd.read_html(data.text, match="Goalkeeping")[0]
goalkeeping.columns = goalkeeping.columns.droplevel()


goalkeeping.head()


soup = BeautifulSoup(data.text)
links = soup.find_all('a')
links = [l.get("href") for l in links]
links = [l for l in links if l and 'all_comps/passing/' in l]
data = requests.get(f"https://fbref.com{links[0]}")
passing = pd.read_html(data.text, match="Passing")[0]



def add_prefix_to_columns(df):
    upper_level = df.columns.get_level_values(0)
    new_columns = [f'{prefix}_{col}' for prefix, col in zip(upper_level, df.columns.get_level_values(1))]
    df.columns = new_columns

# Apply the function to add prefixes
add_prefix_to_columns(passing)


passing.head()


passing.columns


passing.rename(columns={passing.columns[0]: 'Date'}, inplace=True)


soup = BeautifulSoup(data.text)
links = soup.find_all('a')
links = [l.get("href") for l in links]
links = [l for l in links if l and 'all_comps/possession/' in l]
data = requests.get(f"https://fbref.com{links[0]}")
possession = pd.read_html(data.text, match="Possession")[0]
possession.columns = possession.columns.droplevel()


possession.head()


Scores_Fixtures = Scores_Fixtures[['Date','Time','Comp','Round','Day','Venue','GF','GA','Opponent']]


all_matches = Scores_Fixtures.merge(shooting[['Date','Opponent','Sh','SoT']], on = ['Date'])
all_matches = all_matches.merge(goalkeeping[['Date','Save%']], on = ['Date'])
all_matches = all_matches.merge(passing[['Date','Total_Att','Total_Cmp%']], on = ['Date'])
all_matches = all_matches.merge(possession[['Date','Touches','Att','Carries','Rec']], on = ['Date'])
all_matches.head()


all_matches.shape


years = list(range(2023, 2021, -1))
all_matches = []


import time
for year in years:
    data = requests.get(standings_url)
    soup = BeautifulSoup(data.text)
    standings_table = soup.select('table.stats_table')[0]

    links = [l.get("href") for l in standings_table.find_all('a')]
    links = [l for l in links if '/squads/' in l]
    team_urls = [f"https://fbref.com{l}" for l in links]
    
    previous_season = soup.select("a.prev")[0].get("href")
    standings_url = f"https://fbref.com{previous_season}"
    
    for team_url in team_urls:
        team_name = team_url.split("/")[-1].replace("-Stats", "").replace("-", " ")
        data = requests.get(team_url)
        Scores_Fixtures = pd.read_html(data.text, match="Scores & Fixtures")[0]
        Scores_Fixtures = Scores_Fixtures[['Date','Time','Comp','Round','Day','Venue','GF','GA','Opponent']]
        
        soup = BeautifulSoup(data.text)
        links = [l.get("href") for l in soup.find_all('a')]
        links = [l for l in links if l and 'all_comps/shooting/' in l]
        data = requests.get(f"https://fbref.com{links[0]}")
        shooting = pd.read_html(data.text, match="Shooting")[0]
        shooting.columns = shooting.columns.droplevel()

        soup = BeautifulSoup(data.text)
        links = soup.find_all('a')
        links = [l.get("href") for l in links]
        links = [l for l in links if l and 'all_comps/keeper/' in l]
        data = requests.get(f"https://fbref.com{links[0]}")
        goalkeeping = pd.read_html(data.text, match="Goalkeeping")[0]
        goalkeeping.columns = goalkeeping.columns.droplevel()

        soup = BeautifulSoup(data.text)
        links = soup.find_all('a')
        links = [l.get("href") for l in links]
        links = [l for l in links if l and 'all_comps/passing/' in l]
        data = requests.get(f"https://fbref.com{links[0]}")
        passing = pd.read_html(data.text, match="Passing")[0]
        upper_level = passing.columns.get_level_values(0)
        new_columns = [f'{prefix}_{col}' for prefix, col in zip(upper_level, passing.columns.get_level_values(1))]
        passing.columns = new_columns
        passing.rename(columns={passing.columns[0]: 'Date'}, inplace=True)

        soup = BeautifulSoup(data.text)
        links = soup.find_all('a')
        links = [l.get("href") for l in links]
        links = [l for l in links if l and 'all_comps/possession/' in l]
        data = requests.get(f"https://fbref.com{links[0]}")
        possession = pd.read_html(data.text, match="Possession")[0]
        possession.columns = possession.columns.droplevel()
        
        #try:
        team_data = Scores_Fixtures.merge(shooting[['Date','Opponent','Sh','SoT']], on = ['Date'])
        team_data = team_data.merge(goalkeeping[['Date','Save%']], on = ['Date'])
        team_data = team_data.merge(passing[['Date','Total_Att','Total_Cmp%']], on = ['Date'])
        team_data = team_data.merge(possession[['Date','Touches','Att','Carries','Rec']], on = ['Date'])
        #except ValueError:
            #continue
        team_data = team_data[team_data["Comp"] == "Bundesliga"]
        
        team_data["Season"] = year
        team_data["Team"] = team_name
        all_matches.append(team_data)
        time.sleep(10)



